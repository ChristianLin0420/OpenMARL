<div align="center">
<h1>RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints</h1>

<a href="https://arxiv.org/pdf/2503.16408" target="_blank" rel="noopener noreferrer">
  <img src="https://img.shields.io/badge/Paper-RoboFactory" alt="Paper PDF">
</a>
<a href="https://arxiv.org/abs/2503.16408"><img src="https://img.shields.io/badge/arxiv-2503.16408-b31b1b" alt="arXiv"></a>
<a href="https://iranqin.github.io/robofactory/"><img src="https://img.shields.io/badge/Project_Page-green" alt="Project Page"></a>
<a href='https://huggingface.co/datasets/FACEONG/RoboFactory_Dataset'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Datasets-blue'></a>
</div>

## Overview
RoboFactory is a benchmark for embodied multi-agent manipulation, based on [ManiSkill](https://www.maniskill.ai/). Leveraging compositional constraints and specifically designed interfaces, it's an automated data collection framework for embodied multi-agent systems.

## Quick Start
First, clone this repository to your local machine, and install the dependencies.
```bash
git clone git@github.com:MARS-EAI/RoboFactory.git
conda create -n RoboFactory python=3.9
conda activate RoboFactory
cd RoboFactory
pip install -r requirements.txt
conda install -c conda-forge networkx=2.5
```
Download the 3D assets in RoboFactory task:
```bash
python script/download_assets.py 
```
Now, try to run the task with just a line of code:
```bash
python script/run_task.py configs/table/lift_barrier.yaml
```
For more complex scene like RoboCasa, you can download them using the following commands. Note that if you use these scenes in your work please cite the scene dataset authors.
```bash
python -m mani_skill.utils.download_asset RoboCasa
```
After download the scene dataset, you can try to run it:
```bash
python script/run_task.py configs/robocasa/lift_barrier.yaml
```
## Generate Data
You can use the following script to generate data. The generated data is usually placed in the demos/ folder.
```bash
# Format: python script/generate_data.py {config_path}
python script/generate_data.py configs/table/lift_barrier.yaml
```
## Train & Evaluate Policy
### Data Processing
The data generated by the ManiSkill framework is in .h5 format. In order to adapt to the training code, we need to convert it to .zarr format. You can convert it according to the following method.
```bash
# 1. make data folder in the first time.
mkdir data
mkdir -p data/{h5_data,pkl_data,zarr_data}

# 2. move your .h5 and .json file into the data/h5_data folder.
mv {your_h5_file}.h5 data/h5_data/{task_name}.h5
mv {your_h5_file}.json data/h5_data/{task_name}.json

# 3. run the script to process the data.
# NOTE: This is the script for default config. If you add the additional camera in config yaml, modify the script to adapt the data.
# Example:
python script/parse_h5_to_pkl_multi.py --task_name LiftBarrier-rf --load_num 150  --agent_num 2
# For 2 agents task, convert 2 .pkl file into .zarr file respectively.
# Example:
python script/parse_pkl_to_zarr_dp.py --task_name LiftBarrier-rf --load_num 150 --agent_id 0
python script/parse_pkl_to_zarr_dp.py --task_name LiftBarrier-rf --load_num 150 --agent_id 1
```
### Train
We currently provide training code for Diffusion Policy (DP), and we plan to provide more Policy in the future.
You can train the DP model through the following code:
```bash
bash policy/Diffusion-Policy/train.sh ${task_name} ${load_num} ${agent_id} ${seed} ${gpu_id}
# Example:
bash policy/Diffusion-Policy/train.sh LiftBarrier-rf 150 1 100 0
```
### Evaluation
Use the .ckpt file to evaluate your model results after the training is completed. When setting DEBUG_MODE to 1, it will open the visual window and output more info.
```bash
bash policy/Diffusion-Policy/eval_multi.sh ${config_name} ${DATA_NUM} ${CHECKPOINT_NUM} ${DEBUG_MODE} ${TASK_NAME}
# Example
bash policy/Diffusion-Policy/eval_multi.sh configs/table/lift_barrier.yaml 150 300 1 LiftBarrier-rf
```
### BibTeX
```bibtex
@article{qin2025robofactory,
  title={RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints},
  author={Qin, Yiran and Kang, Li and Song, Xiufeng and Yin, Zhenfei and Liu, Xiaohong and Liu, Xihui and Zhang, Ruimao and Bai, Lei},
  journal={arXiv preprint arXiv:2503.16408},
  year={2025}
}
```
